{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243ad852fc68d7d2",
   "metadata": {},
   "source": [
    "# C1 W2 Group 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import wiener\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "from src.data import GT_QSD1_W3_LIST\n",
    "from src.paths import (\n",
    "    BBDD_PATH, \n",
    "    QSD1_W3_PATH, \n",
    "    QSD1_NON_AUGMENTED_W3_PATH, \n",
    "    WEEK_3_PATH, \n",
    "    WEEK_3_RESULTS_PATH\n",
    ")\n",
    "from src.similarities import HistogramIntersection\n",
    "from src.metrics import MeanAveragePrecisionAtK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f0c3825c7f479",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657424863441b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_image_PIL_list = [Image.open(db_img_path) for db_img_path in sorted(BBDD_PATH.glob(\"*.jpg\"))]  # Load once\n",
    "for idx, db_img in enumerate(database_image_PIL_list):\n",
    "    assert db_img.filename.endswith(f\"{idx}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988536713d7476e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert Roca\\Documents\\MCV\\C1 Image UPC\\Project\\Team8\\data\\qsd1_w3\n"
     ]
    }
   ],
   "source": [
    "print(QSD1_W3_PATH)\n",
    "query_d1_image_PIL_list = [Image.open(query_img_path) for query_img_path in sorted(QSD1_W3_PATH.glob(\"*.jpg\"))]  # Load once\n",
    "for idx, query_img in enumerate(query_d1_image_PIL_list):\n",
    "    assert query_img.filename.endswith(f\"{idx}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2c41d6d19ebb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_augmented_d1_image_PIL_list = [Image.open(query_img_path) for query_img_path in sorted(QSD1_NON_AUGMENTED_W3_PATH.glob(\"*.jpg\"))]  # Load once\n",
    "for idx, query_img in enumerate(non_augmented_d1_image_PIL_list):\n",
    "    assert query_img.filename.endswith(f\"{idx}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fdc66345e06a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_images_d1 = []\n",
    "noisy_images_indexes_d1 = []\n",
    "augmentations = []\n",
    "with (QSD1_W3_PATH / \"augmentations.pkl\").open('rb') as f:\n",
    "    augmentations = pickle.load(f)\n",
    "    for idx, img in enumerate(augmentations):\n",
    "        if img != 'None':\n",
    "            noisy_images_d1.append(query_d1_image_PIL_list[idx])\n",
    "            noisy_images_indexes_d1.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491c261d0039b14",
   "metadata": {},
   "source": [
    "## Task 1: Noise filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "539e6a9318f3bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions to handle RGB images\n",
    "def split_channels(image):\n",
    "    \"\"\"Split RGB image into its three color channels.\"\"\"\n",
    "    return cv2.split(image)\n",
    "\n",
    "def merge_channels(channels):\n",
    "    \"\"\"Merge the denoised RGB channels back together and clip to valid range [0, 255].\"\"\"\n",
    "    merged_image = cv2.merge(channels)\n",
    "    return np.clip(merged_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "# --- Linear Filters ---\n",
    "\n",
    "def gaussian_blur(image, kernel_size=(3, 3)):\n",
    "    channels = split_channels(image)\n",
    "    denoised_channels = [cv2.GaussianBlur(channel, kernel_size, 0) for channel in channels]\n",
    "    return merge_channels(denoised_channels)\n",
    "\n",
    "def mean_filter(image, kernel_size=(3, 3)):\n",
    "    channels = split_channels(image)\n",
    "    denoised_channels = [cv2.blur(channel, kernel_size) for channel in channels]\n",
    "    return merge_channels(denoised_channels)\n",
    "\n",
    "def wiener_filter(image, kernel_size=3):\n",
    "    channels = split_channels(image)\n",
    "    denoised_channels = []\n",
    "    \n",
    "    for channel in channels:\n",
    "        denoised_channel = wiener(channel, kernel_size)\n",
    "        denoised_channel[np.isnan(denoised_channel)] = 0 \n",
    "        denoised_channel[denoised_channel < 0] = 0  \n",
    "        denoised_channels.append(denoised_channel)\n",
    "    \n",
    "    return merge_channels(denoised_channels)\n",
    "\n",
    "# --- Non-Linear Filters ---\n",
    "\n",
    "def median_filter(image, kernel_size=3):\n",
    "    channels = split_channels(image)\n",
    "    denoised_channels = [cv2.medianBlur(channel, kernel_size) for channel in channels]\n",
    "    return merge_channels(denoised_channels)\n",
    "\n",
    "def bilateral_filter(image, diameter=5, sigma_color=25, sigma_space=25):\n",
    "    channels = split_channels(image)\n",
    "    denoised_channels = [cv2.bilateralFilter(channel, diameter, sigma_color, sigma_space) for channel in channels]\n",
    "    return merge_channels(denoised_channels)\n",
    "\n",
    "def non_local_means_filter(image, h=0.6, patch_size=3, patch_distance=5, fast_mode=True):\n",
    "    sigma_est = np.mean([estimate_sigma(image[..., channel], channel_axis=None) for channel in range(image.shape[-1])])\n",
    "    denoised_image = denoise_nl_means(image, h=h * sigma_est, \n",
    "                                      patch_size=patch_size, \n",
    "                                      patch_distance=patch_distance,\n",
    "                                      channel_axis=-1, fast_mode=fast_mode)\n",
    "    return merge_channels([denoised_image[..., i] * 255 for i in range(denoised_image.shape[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d9faaa9d3d7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_quality(denoised_image, ground_truth):\n",
    "    psnr_value = psnr(ground_truth, denoised_image, data_range=255)\n",
    "    ssim_value, _ = ssim(ground_truth, denoised_image, full=True, channel_axis=-1)\n",
    "    return psnr_value, ssim_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3715981b7f1f6e90",
   "metadata": {},
   "source": [
    "### Grid search\n",
    "We perform a grid search to find the optimal parameters for each of the filters, evaluating it with the noisy images of `qsd1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce05ed2ec0876fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising methods with parameter exploration\n",
    "def plot_results(metric_values, methods_names, metric_name, params):\n",
    "    #  Convert tuple params like (3, 3) into single values like 3, 5, 7\n",
    "    kernel_sizes = [param[0] for param in params]\n",
    "    num_methods = len(methods_names)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bar_width = 0.15\n",
    "    indices = np.arange(len(kernel_sizes))\n",
    "\n",
    "    for i, method in enumerate(methods_names):\n",
    "        plt.bar(indices + i * bar_width, metric_values[i], bar_width, label=method)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Kernel Sizes')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f'{metric_name} for Denoising Methods with Different Kernel Sizes')\n",
    "    plt.xticks(indices + bar_width * (num_methods - 1) / 2, kernel_sizes)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def explore_filter(filter_func, param_combinations, image, ground_truth):\n",
    "    psnr_values, ssim_values = [], []\n",
    "    best_psnr, best_ssim, best_params = -float('inf'), -float('inf'), None\n",
    "    for params in param_combinations:\n",
    "        denoised_image = filter_func(image, *params)\n",
    "        psnr_value, ssim_value = evaluate_image_quality(denoised_image, ground_truth)\n",
    "        psnr_values.append(psnr_value)\n",
    "        ssim_values.append(ssim_value)\n",
    "        if psnr_value > best_psnr and ssim_value > best_ssim:\n",
    "            best_psnr, best_ssim, best_params = psnr_value, ssim_value, params\n",
    "    return best_params, best_psnr, best_ssim, psnr_values, ssim_values, param_combinations\n",
    "\n",
    "def explore_gaussian_blur(image, ground_truth, kernel_sizes):\n",
    "    return explore_filter(gaussian_blur, [(k,) for k in kernel_sizes], image, ground_truth)\n",
    "\n",
    "def explore_mean_filter(image, ground_truth, kernel_sizes):\n",
    "    return explore_filter(mean_filter, [(k,) for k in kernel_sizes], image, ground_truth)\n",
    "\n",
    "def explore_wiener_filter(image, ground_truth, kernel_sizes):\n",
    "    return explore_filter(wiener_filter, [(k,) for k in kernel_sizes], image, ground_truth)\n",
    "\n",
    "def explore_median_filter(image, ground_truth, kernel_sizes):\n",
    "    return explore_filter(median_filter, [(k,) for k in kernel_sizes], image, ground_truth)\n",
    "\n",
    "def explore_bilateral_filter(image, ground_truth, diameters, sigma_colors, sigma_spaces):\n",
    "    return explore_filter(bilateral_filter, product(diameters, sigma_colors, sigma_spaces), image, ground_truth)\n",
    "\n",
    "def explore_nl_means_filter(image, ground_truth, h_values, patch_sizes, patch_distances):\n",
    "    return explore_filter(non_local_means_filter, product(h_values, patch_sizes, patch_distances), image, ground_truth)\n",
    "\n",
    "\n",
    "# Explore all methods and return the best parameters\n",
    "def explore_all_methods(image, ground_truth):\n",
    "    # Parameter ranges for each method\n",
    "    kernel_sizes = [(1,1), (3, 3), (5, 5), (7, 7), (9, 9), (11, 11)]\n",
    "\n",
    "    diameters = [5, 9, 13, 17, 21]\n",
    "    sigma_colors = [25, 50, 75, 100, 125]\n",
    "    sigma_spaces = [25, 50, 75, 100, 125]\n",
    "    h_values = [0.6, 0.8, 1.0, 1.2, 1.5]\n",
    "    patch_sizes = [3, 5, 7, 9, 11]\n",
    "    patch_distances = [5, 6, 8, 11, 17]\n",
    "\n",
    "    all_results = {}\n",
    "    best_results = {}\n",
    "\n",
    "    # Explore each denoising method\n",
    "    all_results['Gaussian'] = explore_gaussian_blur(image, ground_truth, kernel_sizes)\n",
    "    best_results['Gaussian'] = all_results['Gaussian'][0:3]\n",
    "    all_results['Mean'] = explore_mean_filter(image, ground_truth, kernel_sizes)\n",
    "    best_results['Mean'] = all_results['Mean'][0:3]\n",
    "    all_results['Wiener'] = explore_wiener_filter(image, ground_truth, kernel_sizes)\n",
    "    best_results['Wiener'] = all_results['Wiener'][0:3]\n",
    "    all_results['Median'] = explore_median_filter(image, ground_truth, kernel_sizes)\n",
    "    best_results['Median'] = all_results['Median'][0:3]\n",
    "    '''all_results['Bilateral'] = explore_bilateral_filter(image, ground_truth, diameters, sigma_colors, sigma_spaces)\n",
    "    best_results['Bilateral'] = all_results['Bilateral'][0:3]\n",
    "    all_results['NLM'] = explore_nl_means_filter(image, ground_truth, h_values, patch_sizes, patch_distances)\n",
    "    best_results['NLM'] = all_results['NLM'][0:3]'''\n",
    "    \n",
    "    methods = []\n",
    "    psnr_results = []\n",
    "    ssim_results = []\n",
    "    for method_name, result in all_results.items():\n",
    "        if method_name in ['Gaussian', 'Mean', 'Wiener', 'Median']:\n",
    "            methods.append(method_name)\n",
    "            psnr_results.append(result[3])\n",
    "            ssim_results.append(result[4])\n",
    "\n",
    "    df_results = pd.DataFrame.from_dict(best_results, orient='index', columns=['Best Parameters', 'Best PSNR', 'Best SSIM'])\n",
    "    \n",
    "    # Uncomment to display the results for each image\n",
    "    plot_results(psnr_results, methods, \"PSNR\", kernel_sizes)\n",
    "    #plot_results(ssim_results, methods, \"SSIM\", kernel_sizes)\n",
    "    #display(df_results)\n",
    "    return df_results\n",
    "\n",
    "# Set to True to explore all parameters, omitted to make execution faster\n",
    "if False:\n",
    "    methods_names = ['Gaussian', 'Mean', 'Wiener', 'Median', 'Bilateral', 'NLM']\n",
    "    for noisy_idx, noisy_img in zip(noisy_images_indexes_d1, noisy_images_d1):\n",
    "        print(\"Image index: \", noisy_idx)\n",
    "        print(\"Augmentation: \",  augmentations[noisy_idx])\n",
    "        image = np.array(noisy_img)  # Noisy image\n",
    "        ground_truth = np.array(non_augmented_d1_image_PIL_list[noisy_idx])  # Ground truth image\n",
    "        df_out = explore_all_methods(image, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52884d2c73f2552",
   "metadata": {},
   "source": [
    "#### Conclusions of optimal filter parameters\n",
    "\n",
    "What we have found is that it depends a lot on the image the selection of the filter and the kernel size. However, observing the plots below we can observe that in the cases in which an Impulse Noise has been added, in most of the cases the best working filter to remove the noise is the Median with kernel size 3x3. Therefore, we consider it to be the optimal one.\n",
    "\n",
    "### Denoising QSD1\n",
    "\n",
    "In this part we filter all the images with all the filters and the optimal parameters we found by the grid search method, despite the variance of the parameters depending on the image. Then we compute the averages over all the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee505835f963f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the filters to all the images\n",
    "q_results_gaussian = []\n",
    "q_results_mean = []\n",
    "q_results_wiener = []\n",
    "q_results_median = []\n",
    "q_results_bilateral = []\n",
    "q_results_nl_means = []\n",
    "\n",
    "for q_img in tqdm(query_d1_image_PIL_list):\n",
    "    q_img_np = np.array(q_img) \n",
    "    q_results_gaussian.append(gaussian_blur(q_img_np))\n",
    "    q_results_mean.append(mean_filter(q_img_np))\n",
    "    q_results_wiener.append(wiener_filter(q_img_np))\n",
    "    q_results_median.append(median_filter(q_img_np))\n",
    "    q_results_bilateral.append(bilateral_filter(q_img_np))\n",
    "    q_results_nl_means.append(non_local_means_filter(q_img_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598ffef4ccd20e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_multiple_filters(original_images, noisy_images, results_gaussian, results_mean, \n",
    "                                      results_wiener, results_median, \n",
    "                                      results_bilateral, results_nl_means):\n",
    "    num_images = len(original_images)\n",
    "    num_methods = 6 \n",
    "    fig, axes = plt.subplots(num_images, num_methods + 2, figsize=(25, 5 * num_images))  # +1 for original image\n",
    "\n",
    "    for i in range(num_images):\n",
    "        original_image = original_images[i]\n",
    "        noisy_image = noisy_images[i]\n",
    "\n",
    "        axes[i, 0].imshow(original_image)\n",
    "        axes[i, 0].set_title(f'Original Image {i+1}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(noisy_image)\n",
    "        axes[i, 1].set_title(f'Noisy image Image {i+1}')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        axes[i, 2].imshow(results_gaussian[i])\n",
    "        axes[i, 2].set_title(f'Gaussian Denoised {i+1}')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "        axes[i, 3].imshow(results_mean[i])\n",
    "        axes[i, 3].set_title(f'Mean Denoised {i+1}')\n",
    "        axes[i, 3].axis('off')\n",
    "\n",
    "        axes[i, 4].imshow(results_wiener[i])\n",
    "        axes[i, 4].set_title(f'Wiener Denoised {i+1}')\n",
    "        axes[i, 4].axis('off')\n",
    "\n",
    "        axes[i, 5].imshow(results_median[i])\n",
    "        axes[i, 5].set_title(f'Median Denoised {i+1}')\n",
    "        axes[i, 5].axis('off')\n",
    "\n",
    "        axes[i, 6].imshow(results_bilateral[i])\n",
    "        axes[i, 6].set_title(f'Bilateral Denoised {i+1}')\n",
    "        axes[i, 6].axis('off')\n",
    "\n",
    "        axes[i, 7].imshow(results_nl_means[i])\n",
    "        axes[i, 7].set_title(f'NLM Denoised {i+1}')\n",
    "        axes[i, 7].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(WEEK_3_PATH / 'results' / 'denoised_images_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "plot_images_with_multiple_filters(non_augmented_d1_image_PIL_list, query_d1_image_PIL_list, q_results_gaussian, \n",
    "                                  q_results_mean, q_results_wiener, \n",
    "                                  q_results_median, q_results_bilateral, \n",
    "                                  q_results_nl_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560bdfd18f19999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(ground_truth_images, results_gaussian, results_mean, \n",
    "                     results_wiener, results_median, results_bilateral, results_nl_means):\n",
    "    metrics = []\n",
    "    num_images = len(ground_truth_images)\n",
    "\n",
    "    for i in tqdm(range(num_images)):\n",
    "        ground_truth = np.array(ground_truth_images[i])\n",
    "\n",
    "        # Evaluate each denoising method\n",
    "        psnr_gaussian, ssim_gaussian = evaluate_image_quality(results_gaussian[i], ground_truth)\n",
    "        psnr_mean, ssim_mean = evaluate_image_quality(results_mean[i], ground_truth)\n",
    "        psnr_wiener, ssim_wiener = evaluate_image_quality(results_wiener[i], ground_truth)\n",
    "        psnr_median, ssim_median = evaluate_image_quality(results_median[i], ground_truth)\n",
    "        psnr_bilateral, ssim_bilateral = evaluate_image_quality(results_bilateral[i], ground_truth)\n",
    "        psnr_nl_means, ssim_nl_means = evaluate_image_quality(results_nl_means[i], ground_truth)\n",
    "\n",
    "        # Collect the results for each image\n",
    "        metrics.append({\n",
    "            \"Image\": f\"Image {i+1}\",\n",
    "            \"PSNR (Gaussian)\": psnr_gaussian,\n",
    "            \"SSIM (Gaussian)\": ssim_gaussian,\n",
    "            \"PSNR (Mean)\": psnr_mean,\n",
    "            \"SSIM (Mean)\": ssim_mean,\n",
    "            \"PSNR (Wiener)\": psnr_wiener,\n",
    "            \"SSIM (Wiener)\": ssim_wiener,\n",
    "            \"PSNR (Median)\": psnr_median,\n",
    "            \"SSIM (Median)\": ssim_median,\n",
    "            \"PSNR (Bilateral)\": psnr_bilateral,\n",
    "            \"SSIM (Bilateral)\": ssim_bilateral,\n",
    "            \"PSNR (NLM)\": psnr_nl_means,\n",
    "            \"SSIM (NLM)\": ssim_nl_means,\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(metrics)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc812a138fe54e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_all = evaluate_results(non_augmented_d1_image_PIL_list, q_results_gaussian, q_results_mean, \n",
    "                              q_results_wiener, q_results_median, q_results_bilateral, \n",
    "                              q_results_nl_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576fae25cbeef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_noisy = df_results_all[df_results_all['Image'].isin([f\"Image {i+1}\" for i in noisy_images_indexes_d1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07619b55472120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_bar(df_results_all=None, df_results_noisy=None, methods=None):\n",
    "    # Calculate the average PSNR and SSIM for each method\n",
    "    avg_psnr_all = [df_results_all[f\"PSNR ({method})\"].mean() for method in methods]\n",
    "    avg_ssim_all = [df_results_all[f\"SSIM ({method})\"].mean() for method in methods]\n",
    "    avg_psnr_noisy = [df_results_noisy[f\"PSNR ({method})\"].mean() for method in methods]\n",
    "    avg_ssim_noisy = [df_results_noisy[f\"SSIM ({method})\"].mean() for method in methods]\n",
    "\n",
    "    psnr_data = list(zip(methods, avg_psnr_all, avg_psnr_noisy))\n",
    "    psnr_data.sort(key=lambda x: x[1] + x[2], reverse=True)  # Sort by the sum of PSNR values from all and noisy\n",
    "    sorted_methods_psnr, sorted_psnr_all, sorted_psnr_noisy = zip(*psnr_data)\n",
    "    ssim_data = list(zip(methods, avg_ssim_all, avg_ssim_noisy))\n",
    "    ssim_data.sort(key=lambda x: x[1] + x[2], reverse=True)  # Sort by the sum of SSIM values from all and noisy\n",
    "    sorted_methods_ssim, sorted_ssim_all, sorted_ssim_noisy = zip(*ssim_data)\n",
    "    positions = np.arange(len(methods))\n",
    "    bar_width = 0.35\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Bar Plot for PSNR\n",
    "    ax[0].bar(positions - bar_width/2, sorted_psnr_all, bar_width, label='All Images')\n",
    "    ax[0].bar(positions + bar_width/2, sorted_psnr_noisy, bar_width, label='Noisy Images')\n",
    "    ax[0].set_xlabel('Denoising Methods')\n",
    "    ax[0].set_ylabel('Average PSNR')\n",
    "    ax[0].set_title('Comparison of Average PSNR by Denoising Method')\n",
    "    ax[0].set_xticks(positions)\n",
    "    ax[0].set_xticklabels(sorted_methods_psnr)\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Bar Plot for SSIM\n",
    "    ax[1].bar(positions - bar_width/2, sorted_ssim_all, bar_width, label='All Images')\n",
    "    ax[1].bar(positions + bar_width/2, sorted_ssim_noisy, bar_width, label='Noisy Images')\n",
    "    ax[1].set_xlabel('Denoising Methods')\n",
    "    ax[1].set_ylabel('Average SSIM')\n",
    "    ax[1].set_title('Comparison of Average SSIM by Denoising Method')\n",
    "    ax[1].set_xticks(positions)\n",
    "    ax[1].set_xticklabels(sorted_methods_ssim)\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "methods_names = [\"Gaussian\", \"Mean\", \"Wiener\", \"Median\", \"Bilateral\", \"NLM\"]\n",
    "plot_metrics_bar(df_results_all, df_results_noisy, methods_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82341db24d18f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the previous results we set the optimal denoised images\n",
    "query_d1_denoised_images = q_results_median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fa8e1e3402883",
   "metadata": {},
   "source": [
    "#### Conclusions after denoising QSD1\n",
    "\n",
    "As we can observe, the best performing denoising filter is the Median, achieving the highest PSNR and SSIM, with a Kernel size of 3x3. However, the Gaussian performance is extremely close to the Median, as we saw in the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67305f377822990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_row = df_results_all.mean(numeric_only=True)\n",
    "mean_row['Image'] = 'Average'\n",
    "mean_row_df = pd.DataFrame([mean_row])\n",
    "df_results_all_final = pd.concat([df_results_all, mean_row_df], ignore_index=True)\n",
    "df_results_all_final.to_csv(WEEK_3_RESULTS_PATH/\"metrics_all.csv\", index=False, decimal=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f91739505946861",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_row = df_results_noisy.mean(numeric_only=True)\n",
    "mean_row['Image'] = 'Average'\n",
    "mean_row_df = pd.DataFrame([mean_row])\n",
    "df_results_noisy_final = pd.concat([df_results_noisy, mean_row_df], ignore_index=True)\n",
    "df_results_noisy_final.to_csv(WEEK_3_RESULTS_PATH/\"metrics_noisy.csv\", index=False, decimal=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fcdeee3c2afb8",
   "metadata": {},
   "source": [
    "## Task 2: Texture descriptors\n",
    "All descriptors of the project have been implemented in `src/descriptors.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da202952",
   "metadata": {},
   "source": [
    "FOR NOW ONLY DONE ON QUERY IMAGES NON DENOISED NOR NON_AUGMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9179b210f7b3a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.descriptors import LBPDescriptor, DCTDescriptor, WaveletDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea526e265e87211",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture_descriptors = [\n",
    "    WaveletDescriptor(wavelet='haar', level=3),  #triga molt poc\n",
    "    WaveletDescriptor(wavelet='db1',  level=4),  #triga molt poc\n",
    "    LBPDescriptor(num_points=8, radius=1),   # triga mig\n",
    "    # LBPDescriptor(num_points=24, radius=3),  # triga molt\n",
    "    DCTDescriptor(N=10),                     # triga poc\n",
    "    # DCTDescriptor(N=21),                     # triga poc\n",
    "    DCTDescriptor(N=36),                     # triga poc\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913d06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_levels = [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18ba2d",
   "metadata": {},
   "source": [
    "To make the execution faster we persist the partitions of the images for the next runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873e523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images at level 5:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images at level 5: 100%|██████████| 30/30 [00:01<00:00, 17.39it/s]\n",
      "Loading images at level 5: 100%|██████████| 287/287 [00:24<00:00, 11.49it/s]\n"
     ]
    }
   ],
   "source": [
    "def partition_image(image: Image.Image, N: int):\n",
    "    w, h = image.size\n",
    "    part_width, part_height = w // N, h // N\n",
    "    return [image.crop((col * part_width, row * part_height,\n",
    "                        (col + 1) * part_width, (row + 1) * part_height))\n",
    "            for row in range(N) for col in range(N)]\n",
    "\n",
    "\n",
    "def process_partitioned_images(path, PIL_list, partition_levels, mode='auto'):\n",
    "    partitioned_images = {}\n",
    "    \n",
    "    for partition_level in partition_levels:\n",
    "        partition_level_dir = path.with_name(f\"{path.stem}_level_{partition_level}{path.suffix}\")\n",
    "\n",
    "        # Load existing partitions from disk if they exist and mode allows loading\n",
    "        if mode != 'compute' and partition_level_dir.exists():\n",
    "            partitioned_images[partition_level] = []\n",
    "\n",
    "            for img_idx in tqdm(range(len(PIL_list)), desc=f\"Loading images at level {partition_level}\"): \n",
    "                partitions = []\n",
    "                block_idx = 0\n",
    "                while True:\n",
    "                    img_path = partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\"\n",
    "                    if not img_path.exists():\n",
    "                        break  \n",
    "                    with Image.open(img_path) as img:  # Use context manager\n",
    "                        partitions.append(img.copy())\n",
    "                    block_idx += 1\n",
    "\n",
    "                partitioned_images[partition_level].append(partitions)\n",
    "\n",
    "            continue  # Skip computation for this level\n",
    "\n",
    "        # If partitions don't exist, or if mode is 'compute', calculate and store partitions\n",
    "        partition_level_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if partition_level == 1:\n",
    "            print(\"Partitioning at level 1\")\n",
    "            partitioned_images[partition_level] = [[img] for img in PIL_list]\n",
    "        else:\n",
    "            partitioned_images[partition_level] = [\n",
    "                partition_image(img, partition_level) \n",
    "                for img in tqdm(PIL_list, desc=f\"Partitioning at level {partition_level}\")\n",
    "            ]\n",
    "\n",
    "        # Save computed partitions to disk\n",
    "        for img_idx, partitions in tqdm(enumerate(partitioned_images[partition_level]), \n",
    "                                        total=len(partitioned_images[partition_level]), \n",
    "                                        desc=f\"Saving images at level {partition_level}\"):\n",
    "            for block_idx, block_img in enumerate(partitions):\n",
    "                block_img.save(partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\")\n",
    "\n",
    "    return partitioned_images\n",
    "\n",
    "\n",
    "partitioned_images_query = process_partitioned_images(WEEK_3_RESULTS_PATH/\"partitioned_query\",query_d1_image_PIL_list, partition_levels)\n",
    "partitioned_images_db = process_partitioned_images(WEEK_3_RESULTS_PATH/\"partitioned_db\",database_image_PIL_list, partition_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba107a7",
   "metadata": {},
   "source": [
    "The next cell can take over 30 minutes to run !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c73d9b66488f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor:  Wavelet_haar_lvl_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing partitions at level 5: 100%|██████████| 287/287 [02:25<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor:  Wavelet_db1_lvl_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing partitions at level 5: 100%|██████████| 287/287 [01:55<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor:  LBP_np_8_r_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing partitions at level 5: 100%|██████████| 287/287 [18:53<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor:  DCT_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing partitions at level 5: 100%|██████████| 287/287 [20:28<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor:  DCT_36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing partitions at level 5:  19%|█▉        | 54/287 [03:29<18:29,  4.76s/it]"
     ]
    }
   ],
   "source": [
    "def process_partitioned_histograms(descriptors, partition_levels, partitioned_images):\n",
    "    partitioned_histograms = {}\n",
    "\n",
    "    for descriptor in descriptors:\n",
    "        print(\"Descriptor: \", descriptor.name)\n",
    "        partitioned_histograms[descriptor.name] = {}\n",
    "\n",
    "        for partition_level in partition_levels:\n",
    "            partitioned_histograms[descriptor.name][partition_level] = []\n",
    "\n",
    "            for partitions in tqdm(partitioned_images[partition_level], desc=f\"Processing partitions at level {partition_level}\"):\n",
    "                histograms_img = []\n",
    "                for partition_img in partitions:\n",
    "                    histogram_partition = descriptor.compute(np.array(partition_img))\n",
    "                    histograms_img.append(histogram_partition)\n",
    "\n",
    "                concatenated_histogram = np.concatenate(histograms_img, axis=0)\n",
    "                partitioned_histograms[descriptor.name][partition_level].append(concatenated_histogram)\n",
    "\n",
    "    return partitioned_histograms\n",
    "\n",
    "def save_load_histograms(path, compute_func, *args):\n",
    "    if path.exists():\n",
    "        return load_histograms(path)\n",
    "    else:\n",
    "        histograms = compute_func(*args)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(histograms, f)\n",
    "        return histograms\n",
    "\n",
    "def load_histograms(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "partitioned_histograms_query = save_load_histograms(WEEK_3_RESULTS_PATH/\"partitioned_histograms_query.pkl\", process_partitioned_histograms, texture_descriptors, partition_levels, partitioned_images_query)\n",
    "partitioned_histograms_db = save_load_histograms(WEEK_3_RESULTS_PATH/\"partitioned_histograms_db.pkl\", process_partitioned_histograms, texture_descriptors, partition_levels, partitioned_images_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1cfc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_classes = [\n",
    "    HistogramIntersection()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb836e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "for i in partitioned_histograms_query['LBP_np_8_r_1'][5]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9127548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- HistogramIntersection & LBP_np_8_r_1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (287,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Compute BB similarities for each partition level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m partition_level \u001b[38;5;129;01min\u001b[39;00m partition_levels:\n\u001b[1;32m---> 15\u001b[0m     partitioned_db_desc \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartitioned_histograms_db\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdescriptor_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpartition_level\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     partitioned_query_desc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(partitioned_histograms_query[descriptor_name][partition_level])\n\u001b[0;32m     18\u001b[0m     bb_similarity \u001b[38;5;241m=\u001b[39m similarity\u001b[38;5;241m.\u001b[39mcompute(partitioned_query_desc, partitioned_db_desc)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (287,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "query_descriptor_distances_to_db_list = {}\n",
    "\n",
    "for similarity in similarity_classes:\n",
    "    similarity_name = similarity.__class__.__name__\n",
    "    query_descriptor_distances_to_db_list[similarity_name] = {}\n",
    "\n",
    "    for descriptor in texture_descriptors: \n",
    "        descriptor_name = descriptor.name\n",
    "        print(f\"- {similarity_name} & {descriptor_name}\")\n",
    "\n",
    "        query_descriptor_distances_to_db_list[similarity_name][descriptor_name] = {}\n",
    "        \n",
    "        # Compute BB similarities for each partition level\n",
    "        for partition_level in partition_levels:\n",
    "            partitioned_db_desc = np.array(partitioned_histograms_db[descriptor_name][partition_level])\n",
    "            partitioned_query_desc = np.array(partitioned_histograms_query[descriptor_name][partition_level])\n",
    "            \n",
    "            bb_similarity = similarity.compute(partitioned_query_desc, partitioned_db_desc)\n",
    "            query_descriptor_distances_to_db_list[similarity_name][descriptor_name][partition_level] = bb_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b5f1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_distances(query_distances_to_bbdd: np.array, k: int = 1) -> tuple[list[list], list[list]]:\n",
    "    retrieved_bbdd_indices = np.argsort(query_distances_to_bbdd, axis=1)[:, :k]\n",
    "    \n",
    "    retrieved_bbdd_similarity = np.take_along_axis(query_distances_to_bbdd, retrieved_bbdd_indices, axis=1)\n",
    "    \n",
    "    return retrieved_bbdd_indices.tolist(), retrieved_bbdd_similarity.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e87d473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistogramIntersection LBP_np_8_r_1\n",
      "Top-5 for HistogramIntersection - LBP_np_8_r_1 (BB Level 5):\n",
      "Indices: [[7, 155, 4, 30, 112], [65, 251, 14, 275, 148], [128, 158, 285, 173, 188], [137, 81, 239, 23, 215], [193, 27, 259, 203, 265], [110, 156, 200, 37, 105], [110, 156, 200, 21, 37], [272, 263, 277, 252, 185], [13, 255, 222, 162, 262], [133, 157, 105, 37, 205], [286, 123, 72, 70, 120], [215, 193, 265, 51, 32], [259, 220, 32, 115, 212], [89, 177, 222, 49, 206], [219, 179, 75, 38, 67], [135, 63, 167, 103, 161], [110, 156, 200, 37, 105], [104, 193, 161, 259, 57], [110, 200, 156, 37, 21], [82, 203, 22, 32, 31], [110, 156, 200, 37, 21], [81, 39, 61, 88, 69], [157, 76, 43, 249, 212], [110, 21, 37, 200, 25], [264, 11, 226, 279, 65], [120, 70, 51, 40, 39], [30, 129, 172, 4, 284], [25, 110, 200, 21, 93], [93, 110, 200, 227, 187], [110, 200, 25, 37, 21]]\n",
      "\n",
      "\n",
      "HistogramIntersection DCT_10\n",
      "Top-5 for HistogramIntersection - DCT_10 (BB Level 5):\n",
      "Indices: [[7, 30, 250, 53, 164], [186, 1, 250, 92, 53], [36, 93, 54, 40, 35], [35, 65, 238, 54, 177], [262, 23, 246, 176, 75], [23, 90, 184, 258, 1], [21, 40, 36, 54, 93], [272, 36, 54, 40, 209], [13, 1, 250, 108, 266], [133, 252, 236, 65, 107], [286, 184, 1, 250, 278], [22, 1, 194, 250, 6], [91, 248, 60, 189, 47], [222, 36, 238, 54, 35], [219, 179, 53, 55, 250], [248, 40, 183, 47, 36], [94, 255, 237, 176, 97], [104, 74, 1, 250, 164], [164, 250, 1, 266, 53], [201, 266, 1, 194, 164], [252, 168, 101, 102, 247], [270, 211, 81, 189, 277], [280, 250, 1, 106, 194], [131, 236, 165, 250, 74], [258, 184, 90, 204, 1], [120, 40, 36, 54, 209], [30, 250, 1, 164, 107], [25, 1, 250, 164, 92], [93, 36, 11, 222, 54], [130, 184, 1, 204, 6]]\n",
      "\n",
      "\n",
      "HistogramIntersection DCT_30\n",
      "Top-5 for HistogramIntersection - DCT_30 (BB Level 5):\n",
      "Indices: [[7, 30, 53, 250, 1], [186, 1, 250, 53, 74], [128, 36, 30, 93, 47], [35, 250, 177, 164, 1], [262, 1, 176, 6, 250], [23, 1, 184, 194, 204], [36, 54, 11, 40, 238], [272, 250, 1, 164, 107], [13, 1, 250, 266, 53], [133, 252, 277, 218, 107], [286, 184, 1, 250, 124], [22, 1, 250, 194, 6], [91, 102, 47, 189, 52], [222, 250, 1, 189, 238], [219, 179, 53, 1, 250], [248, 177, 36, 236, 250], [94, 1, 194, 184, 250], [104, 1, 250, 74, 164], [164, 250, 1, 53, 107], [201, 1, 250, 164, 266], [252, 101, 189, 102, 250], [81, 36, 11, 93, 54], [280, 1, 250, 164, 194], [131, 236, 250, 74, 107], [258, 184, 204, 1, 250], [120, 1, 36, 250, 40], [30, 250, 1, 164, 53], [25, 1, 250, 164, 53], [93, 36, 238, 11, 54], [184, 1, 250, 6, 130]]\n",
      "\n",
      "\n",
      "HistogramIntersection DCT_100\n",
      "Top-5 for HistogramIntersection - DCT_100 (BB Level 5):\n",
      "Indices: [[7, 1, 53, 250, 164], [186, 1, 250, 53, 164], [128, 11, 36, 107, 30], [35, 250, 1, 164, 177], [262, 1, 250, 6, 53], [23, 1, 204, 250, 194], [21, 11, 36, 54, 1], [272, 36, 1, 164, 250], [1, 250, 13, 53, 164], [107, 252, 133, 250, 74], [286, 1, 250, 124, 53], [22, 1, 250, 6, 53], [91, 102, 47, 36, 189], [222, 36, 1, 250, 238], [219, 179, 53, 1, 250], [248, 54, 36, 1, 236], [94, 1, 194, 250, 164], [104, 74, 250, 238, 1], [164, 1, 250, 53, 124], [201, 1, 250, 164, 53], [252, 1, 250, 107, 164], [81, 36, 11, 40, 93], [280, 250, 1, 164, 107], [131, 74, 250, 55, 107], [258, 184, 1, 204, 250], [120, 36, 1, 250, 164], [30, 1, 250, 164, 53], [25, 1, 250, 164, 53], [93, 36, 11, 1, 250], [184, 1, 250, 6, 53]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define k (number of top results to retrieve)\n",
    "k = 5\n",
    "\n",
    "retrieved_db = {\n",
    "\n",
    "}\n",
    "\n",
    "for similarity_name, descriptors_dict in query_descriptor_distances_to_db_list.items():\n",
    "    retrieved_db[similarity_name] = {}\n",
    "    for descriptor_name, data_dict in descriptors_dict.items():\n",
    "        print(similarity_name, descriptor_name)\n",
    "        retrieved_db[similarity_name][descriptor_name] = {}\n",
    "\n",
    "        # BB Top-k retrieval for each partition level\n",
    "        bb_similarity = data_dict\n",
    "        for partition_level, distances in bb_similarity.items():\n",
    "            retrieved_db[similarity_name][descriptor_name][partition_level] = {}\n",
    "            topk_indices_bb, topk_similarities_bb = get_topk_distances(distances, k)\n",
    "            retrieved_db[similarity_name][descriptor_name][partition_level][\"indexes\"] = topk_indices_bb\n",
    "            retrieved_db[similarity_name][descriptor_name][partition_level][\"similarities\"] = topk_similarities_bb\n",
    "            print(f\"Top-{k} for {similarity_name} - {descriptor_name} (BB Level {partition_level}):\")\n",
    "            print(f\"Indices: {topk_indices_bb}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77c96179",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [MeanAveragePrecisionAtK()]\n",
    "K = [1,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9be4edd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Descriptor</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Method</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>MeanAveragePrecisionAtK</td>\n",
       "      <td>LBP_np_8_r_1</td>\n",
       "      <td>HistogramIntersection</td>\n",
       "      <td>BB at level 5</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MeanAveragePrecisionAtK</td>\n",
       "      <td>DCT_10</td>\n",
       "      <td>HistogramIntersection</td>\n",
       "      <td>BB at level 5</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>MeanAveragePrecisionAtK</td>\n",
       "      <td>DCT_30</td>\n",
       "      <td>HistogramIntersection</td>\n",
       "      <td>BB at level 5</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>MeanAveragePrecisionAtK</td>\n",
       "      <td>DCT_100</td>\n",
       "      <td>HistogramIntersection</td>\n",
       "      <td>BB at level 5</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MeanAveragePrecisionAtK</td>\n",
       "      <td>LBP_np_8_r_1</td>\n",
       "      <td>HistogramIntersection</td>\n",
       "      <td>BB at level 5</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>MeanAveragePrecisionAtK</td>\n",
       "      <td>DCT_10</td>\n",
       "      <td>HistogramIntersection</td>\n",
       "      <td>BB at level 5</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>MeanAveragePrecisionAtK</td>\n",
       "      <td>DCT_30</td>\n",
       "      <td>HistogramIntersection</td>\n",
       "      <td>BB at level 5</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>MeanAveragePrecisionAtK</td>\n",
       "      <td>DCT_100</td>\n",
       "      <td>HistogramIntersection</td>\n",
       "      <td>BB at level 5</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   K                   Metric    Descriptor             Similarity  \\\n",
       "0  1  MeanAveragePrecisionAtK  LBP_np_8_r_1  HistogramIntersection   \n",
       "1  1  MeanAveragePrecisionAtK        DCT_10  HistogramIntersection   \n",
       "2  1  MeanAveragePrecisionAtK        DCT_30  HistogramIntersection   \n",
       "3  1  MeanAveragePrecisionAtK       DCT_100  HistogramIntersection   \n",
       "4  5  MeanAveragePrecisionAtK  LBP_np_8_r_1  HistogramIntersection   \n",
       "5  5  MeanAveragePrecisionAtK        DCT_10  HistogramIntersection   \n",
       "6  5  MeanAveragePrecisionAtK        DCT_30  HistogramIntersection   \n",
       "7  5  MeanAveragePrecisionAtK       DCT_100  HistogramIntersection   \n",
       "\n",
       "          Method  Result  \n",
       "0  BB at level 5    0.43  \n",
       "1  BB at level 5    0.93  \n",
       "2  BB at level 5    0.93  \n",
       "3  BB at level 5    0.90  \n",
       "4  BB at level 5    0.45  \n",
       "5  BB at level 5    0.94  \n",
       "6  BB at level 5    0.94  \n",
       "7  BB at level 5    0.92  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, k in enumerate(K):\n",
    "    for metric in metrics:\n",
    "        for similarity in similarity_classes:\n",
    "            similarity_name = similarity.__class__.__name__\n",
    "            for descriptor in texture_descriptors:\n",
    "                descriptor_name = descriptor.name\n",
    "\n",
    "\n",
    "                # BB\n",
    "                for partition_level in partition_levels:\n",
    "                    indexes_retrieved = retrieved_db[similarity_name][descriptor_name][partition_level][\"indexes\"]\n",
    "                    map_val = round(metric.compute(GT_QSD1_W3_LIST, indexes_retrieved, k), 2)\n",
    "                    results.append({\n",
    "                        \"K\": k,\n",
    "                        \"Metric\": metric.__class__.__name__,\n",
    "                        \"Descriptor\": descriptor_name,\n",
    "                        \"Similarity\": similarity_name,\n",
    "                        \"Method\": f\"BB at level {partition_level}\",\n",
    "                        \"Result\": map_val,\n",
    "                        \"Indices\": indexes_retrieved,\n",
    "                    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df_cleaned = results_df.drop(columns=[\"Indices\", \"Descriptor_id\", \"Similarity_id\"], errors='ignore')\n",
    "\n",
    "results_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76de20f",
   "metadata": {},
   "source": [
    "Without partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47e998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0127013f591b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_qsd1 = {}\n",
    "for descriptor_func in texture_descriptors:\n",
    "    descriptors_qsd1[descriptor_func.__class__.__name__] = []\n",
    "    for image in tqdm(query_d1_image_PIL_list):\n",
    "        descriptors_qsd1[descriptor_func.__class__.__name__].append(descriptor_func.compute(np.array(image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_similarity = HistogramIntersection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
