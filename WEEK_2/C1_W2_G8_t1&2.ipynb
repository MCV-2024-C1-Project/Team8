{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# C1 W2 Group 8",
   "id": "40cc59bf39f6bbae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "\n",
    "from src.data import GT_QSD1_W1_LIST\n",
    "from src.paths import BBDD_PATH, WEEK_2_RESULTS_PATH, QSD1_W1_PATH\n",
    "from src.descriptors import GreyScaleHistogramDescriptor1D, ColorHistogramDescriptor1D, MultiColorSpaceHistogramDescriptor1D, ColorHistogramDescriptor3D\n",
    "from src.similarities import MSE, L1Distance, ChiSquaredDistance, HistogramIntersection, HellingerKernel, Bhattacharyya\n",
    "from src.metrics import MeanAveragePrecisionAtK\n",
    "from tqdm import tqdm\n",
    "import datetime\n"
   ],
   "id": "6ae0579860f63dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1 - Implement 3D / 2D and block and hierarchical histograms\n",
    "\n",
    "### Data processing : Image loading, partitioning & descriptor computation\n",
    "\n",
    "First, we compute the partitions of the images, and the corresponding descriptors"
   ],
   "id": "88b6eac9dee74801"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "database_image_PIL_list = [Image.open(db_img_path) for db_img_path in sorted(BBDD_PATH.glob(\"*.jpg\"))]  # Load once\n",
    "for idx, db_img in enumerate(database_image_PIL_list):\n",
    "    assert db_img.filename.endswith(f\"{idx}.jpg\")"
   ],
   "id": "3aa4a6530a31f935",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query_image_PIL_list = [Image.open(query_img_path) for query_img_path in sorted(QSD1_W1_PATH.glob(\"*.jpg\"))]  # Load once\n",
    "for idx, query_img in enumerate(query_image_PIL_list):\n",
    "    assert query_img.filename.endswith(f\"{idx}.jpg\")"
   ],
   "id": "69993550ce05a787",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def partition_image(image: Image.Image, N: int):\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # Compute info about the partition\n",
    "    rows = cols = int(N)  # Assuming N is a perfect square\n",
    "    part_width = img_width // cols\n",
    "    part_height = img_height // rows\n",
    "    \n",
    "    partitions = []\n",
    "    \n",
    "    # Crop each partition\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            left = col * part_width\n",
    "            top = row * part_height\n",
    "            right = left + part_width\n",
    "            bottom = top + part_height\n",
    "            part = image.crop((left, top, right, bottom))\n",
    "            partitions.append(part)\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "def plot_partitions(image, N):\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # Compute info about the partition\n",
    "    rows = cols = int(N)  # Assuming N is a perfect square\n",
    "    part_width = img_width // cols\n",
    "    part_height = img_height // rows\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Vertical partition lines\n",
    "    for i in range(1, cols):\n",
    "        x = i * part_width\n",
    "        draw.line([(x, 0), (x, img_height)], fill=\"white\", width=5)  \n",
    "    \n",
    "    # Horizontal partition lines\n",
    "    for i in range(1, rows):\n",
    "        y = i * part_height\n",
    "        draw.line([(0, y), (img_width, y)], fill=\"white\", width=5)  \n",
    "    \n",
    "    return image"
   ],
   "id": "8e73ae90d5c5b705",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "partitioned_images_db = {}\n",
    "partition_levels = [1,2,3]"
   ],
   "id": "abe74ad5e1964da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To make the execution faster we persist the partitions of the images for the next runs of the notebook.",
   "id": "5b34a0afb5498d10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for partition_level in partition_levels:\n",
    "    partition_level_dir = WEEK_2_RESULTS_PATH / f\"partitioned_db_images_level_{partition_level}\"\n",
    "    \n",
    "    # If we have previously executed the code, the partitions are stored in the disk and we directly load them\n",
    "    if partition_level_dir.exists():\n",
    "        partitioned_images_db[partition_level] = []\n",
    "\n",
    "        for img_idx in tqdm(range(len(database_image_PIL_list)), desc=f\"Loading images at level {partition_level}\"):\n",
    "            partitions = []\n",
    "            block_idx = 0\n",
    "            while True:\n",
    "                img_path = partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\"\n",
    "                if not img_path.exists():\n",
    "                    break  \n",
    "                partitions.append(Image.open(img_path))\n",
    "                block_idx += 1\n",
    "\n",
    "            partitioned_images_db[partition_level].append(partitions)\n",
    "\n",
    "        continue\n",
    "    \n",
    "    # Otherwise we compute and persist the partitions for next executions\n",
    "    partition_level_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if partition_level == 1:\n",
    "        print(\"Partitioning at level 1\")\n",
    "        partitioned_images_db[partition_level] = [[img] for img in database_image_PIL_list]\n",
    "    else:\n",
    "        partitioned_images_db[partition_level] = [\n",
    "            partition_image(img, partition_level) \n",
    "            for img in tqdm(database_image_PIL_list, desc=f\"Partitioning at level {partition_level}\")\n",
    "        ]\n",
    "    \n",
    "    for img_idx, partitions in tqdm(enumerate(partitioned_images_db[partition_level]), \n",
    "                                    total=len(partitioned_images_db[partition_level]), \n",
    "                                    desc=f\"Saving images at level {partition_level}\"):\n",
    "        for block_idx, block_img in enumerate(partitions):\n",
    "            block_img.save(partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\")\n"
   ],
   "id": "aea7d0439c975970",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "descriptors = [\n",
    "    MultiColorSpaceHistogramDescriptor1D(['HSV', 'LAB', 'YCbCr'], histogram_type='log-chromatic'),\n",
    "    #ColorHistogramDescriptor3D(\"RGB\", 9, histogram_type='log-chromatic'),\n",
    "    ColorHistogramDescriptor3D(\"HSV\", 9, histogram_type='log-chromatic'),\n",
    "    #ColorHistogramDescriptor3D(\"YCbCr\", 9, histogram_type='log-chromatic'),\n",
    "    #ColorHistogramDescriptor3D(\"LAB\", 9, histogram_type='log-chromatic'),\n",
    "]\n",
    "\n",
    "partitioned_histograms_db = {}"
   ],
   "id": "adc930b06647f198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "for descriptor in descriptors:\n",
    "    print(\"Descriptor: \", descriptor.name)\n",
    "    partitioned_histograms_db[descriptor.name] = {}\n",
    "\n",
    "    for partition_level in partition_levels:\n",
    "        print(\"Partition Level: \", partition_level)\n",
    "        partitioned_histograms_db[descriptor.name][partition_level] = []\n",
    "\n",
    "        for partitions in tqdm(partitioned_images_db[partition_level]):\n",
    "            histograms_img = []\n",
    "            for partition_img in partitions:\n",
    "                histogram_partition = descriptor.compute(partition_img)\n",
    "                histograms_img.append(histogram_partition)\n",
    "\n",
    "\n",
    "            concatenated_histogram = np.concatenate(histograms_img, axis=0)\n",
    "                \n",
    "            partitioned_histograms_db[descriptor.name][partition_level].append(concatenated_histogram)\n"
   ],
   "id": "8e1cc07b40abb516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we perform a similar process for the queries",
   "id": "9264b832d9b5fc0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "partitioned_images_query = {}\n",
    "\n",
    "for partition_level in partition_levels:\n",
    "    partition_level_dir = WEEK_2_RESULTS_PATH / f\"partitioned_query_images_level_{partition_level}\"\n",
    "    \n",
    "    # If we have previously executed the code, the partitions are stored in the disk and we directly load them\n",
    "    if partition_level_dir.exists():\n",
    "        partitioned_images_query[partition_level] = []\n",
    "\n",
    "        for img_idx in tqdm(range(len(query_image_PIL_list)), desc=f\"Loading images at level {partition_level}\"):\n",
    "            partitions = []\n",
    "            block_idx = 0\n",
    "            while True:\n",
    "                img_path = partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\"\n",
    "                if not img_path.exists():\n",
    "                    break  \n",
    "                partitions.append(Image.open(img_path))\n",
    "                block_idx += 1\n",
    "\n",
    "            partitioned_images_query[partition_level].append(partitions)\n",
    "\n",
    "        continue\n",
    "    \n",
    "    # Otherwise we compute and persist the partitions for next executions\n",
    "    partition_level_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if partition_level == 1:\n",
    "        print(\"Partitioning at level 1\")\n",
    "        partitioned_images_query[partition_level] = [[img] for img in query_image_PIL_list]\n",
    "    else:\n",
    "        partitioned_images_query[partition_level] = [\n",
    "            partition_image(img, partition_level) \n",
    "            for img in tqdm(query_image_PIL_list, desc=f\"Partitioning at level {partition_level}\")\n",
    "        ]\n",
    "    \n",
    "    for img_idx, partitions in tqdm(enumerate(partitioned_images_query[partition_level]), \n",
    "                                    total=len(partitioned_images_query[partition_level]), \n",
    "                                    desc=f\"Saving images at level {partition_level}\"):\n",
    "        for block_idx, block_img in enumerate(partitions):\n",
    "            block_img.save(partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\")"
   ],
   "id": "13d6168c73fbb44a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "partitioned_histograms_query = {}\n",
    "\n",
    "for descriptor in descriptors:\n",
    "    print(\"Descriptor: \", descriptor.name)\n",
    "    partitioned_histograms_query[descriptor.name] = {}\n",
    "\n",
    "    for partition_level in partition_levels:\n",
    "        print(\"Partition Level: \", partition_level)\n",
    "        partitioned_histograms_query[descriptor.name][partition_level] = []\n",
    "\n",
    "        for partitions in tqdm(partitioned_images_query[partition_level]):\n",
    "            histograms_img = []\n",
    "            for partition_img in partitions:\n",
    "                histogram_partition = descriptor.compute(partition_img)\n",
    "                histograms_img.append(histogram_partition)\n",
    "\n",
    "            concatenated_histogram = np.concatenate(histograms_img, axis=0)\n",
    "            partitioned_histograms_query[descriptor.name][partition_level].append(concatenated_histogram)"
   ],
   "id": "6d27fdb432c94a28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Concatentation strategies",
   "id": "4bffa46f6d0e4bed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we will concatenate the histograms for both cases, the block-based (BB) and the spatial pyramid representation (SPR). We only do that if the histogram has one dimension.",
   "id": "b6fc7945f221dde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "histograms_SPR_DB = {}\n",
    "histograms_SPR_Q = {}\n",
    "histograms_BB_DB = {}\n",
    "histograms_BB_Q = {}\n",
    "\n",
    "for db_data, query_data in zip(partitioned_histograms_db.items(), partitioned_histograms_query.items()):\n",
    "    descriptor_db = db_data[0]\n",
    "    partitions_db = db_data[1]\n",
    "    descriptor_q = query_data[0]\n",
    "    partitions_q = query_data[1]\n",
    "    assert descriptor_db == descriptor_q\n",
    "\n",
    "    histograms_BB_DB[descriptor_db] = partitioned_histograms_db[descriptor_db]\n",
    "    histograms_BB_Q[descriptor_db] = partitioned_histograms_query[descriptor_db]\n",
    "\n",
    "    histograms_SPR_DB[descriptor_db] = []\n",
    "    for img_idx in range(len(database_image_PIL_list)):\n",
    "        image_histograms = []\n",
    "        for partition_level in partition_levels:\n",
    "            image_histograms.append(partitioned_histograms_db[descriptor_db][partition_level][img_idx])\n",
    "\n",
    "        concatenated = np.concatenate(image_histograms, axis=0)\n",
    "        histograms_SPR_DB[descriptor_db].append(concatenated)\n",
    "\n",
    "\n",
    "    histograms_SPR_Q[descriptor_db] = []\n",
    "    for img_idx in range(len(query_image_PIL_list)):\n",
    "        image_histograms = []\n",
    "        for partition_level in partition_levels:\n",
    "            image_histograms.append(partitioned_histograms_query[descriptor_db][partition_level][img_idx])\n",
    "\n",
    "        concatenated = np.concatenate(image_histograms, axis=0)\n",
    "        histograms_SPR_Q[descriptor_db].append(concatenated)\n"
   ],
   "id": "76912b791b6fb658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Similarity computations",
   "id": "e8446149c1d823e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarity_classes = [\n",
    "    #MSE(),\n",
    "    #L1Distance(),\n",
    "    #ChiSquaredDistance(),\n",
    "    HistogramIntersection(),\n",
    "    HellingerKernel(),\n",
    "    Bhattacharyya()\n",
    "]"
   ],
   "id": "2774de91152f7211",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query_descriptor_distances_to_db_list = {}\n",
    "\n",
    "for similarity in similarity_classes:\n",
    "    similarity_name = similarity.__class__.__name__\n",
    "    query_descriptor_distances_to_db_list[similarity_name] = {}\n",
    "    for descriptor in descriptors:\n",
    "        descriptor_name = descriptor.name\n",
    "        print(f\"- {similarity_name} & {descriptor_name}\")\n",
    "        query_descriptor_distances_to_db_list[similarity_name][descriptor_name] = {\n",
    "            \"BB\": {}, # We will compute one similarity per partition level\n",
    "            \"SPR\": [] # We will compute one similarity per image\n",
    "        }\n",
    "        query_descriptors = np.array(histograms_SPR_Q[descriptor_name])\n",
    "        database_descriptors = np.array(histograms_SPR_DB[descriptor_name])\n",
    "\n",
    "        # COMPUTE SIMILARITIES\n",
    "        spr_similarity = similarity.compute(query_descriptors, database_descriptors)\n",
    "        query_descriptor_distances_to_db_list[similarity_name][descriptor_name][\"SPR\"] = spr_similarity\n",
    "\n",
    "        for partition_level in partition_levels:\n",
    "            partitioned_database_descriptors = np.array(histograms_BB_DB[descriptor_name][partition_level])\n",
    "            partitioned_query_descriptors = np.array(histograms_BB_Q[descriptor_name][partition_level])\n",
    "            bb_similarity = similarity.compute(partitioned_query_descriptors, partitioned_database_descriptors)\n",
    "            query_descriptor_distances_to_db_list[similarity_name][descriptor_name][\"BB\"][partition_level] = bb_similarity"
   ],
   "id": "541c2453d4008d16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_distance_matrix(distance_matrix, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(distance_matrix, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Database Images')\n",
    "    plt.ylabel('Query Images')\n",
    "    plt.show()"
   ],
   "id": "b8654b265adb910",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for similarity_name, descriptors_dict in query_descriptor_distances_to_db_list.items():\n",
    "    for descriptor_name, data_dict in descriptors_dict.items():\n",
    "        # Plot the SPR distance matrix\n",
    "        spr_similarity = data_dict[\"SPR\"]\n",
    "        plot_distance_matrix(spr_similarity, f\"{similarity_name} - {descriptor_name} - SPR Distance Matrix\")\n",
    "        \n",
    "        # Plot the BB distance matrices for all partition levels\n",
    "        bb_similarity = data_dict[\"BB\"]\n",
    "        for partition_level, distances in bb_similarity.items():\n",
    "            plot_distance_matrix(distances, f\"{similarity_name} - {descriptor_name} - BB Distance Matrix (Partition Level {partition_level})\")"
   ],
   "id": "9af11bc648c60d6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval",
   "id": "5afaa19673dfe88d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_topk_distances(query_distances_to_bbdd: np.array, k: int = 1) -> tuple[list[list], list[list]]:\n",
    "    retrieved_bbdd_indices = np.argsort(query_distances_to_bbdd, axis=1)[:, :k]\n",
    "    \n",
    "    retrieved_bbdd_similarity = np.take_along_axis(query_distances_to_bbdd, retrieved_bbdd_indices, axis=1)\n",
    "    \n",
    "    return retrieved_bbdd_indices.tolist(), retrieved_bbdd_similarity.tolist()"
   ],
   "id": "8a4b70f615bf15e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define k (number of top results to retrieve)\n",
    "k = 5\n",
    "\n",
    "retrieved_db = {\n",
    "\n",
    "}\n",
    "\n",
    "for similarity_name, descriptors_dict in query_descriptor_distances_to_db_list.items():\n",
    "    retrieved_db[similarity_name] = {}\n",
    "    for descriptor_name, data_dict in descriptors_dict.items():\n",
    "        print(similarity_name, descriptor_name)\n",
    "        retrieved_db[similarity_name][descriptor_name] = {\n",
    "            \"SPR\": {},\n",
    "            \"BB\": {},\n",
    "        }\n",
    "\n",
    "        # SPR Top-k retrieval\n",
    "        spr_similarity = data_dict[\"SPR\"]\n",
    "        topk_indices_spr, topk_similarities_spr = get_topk_distances(spr_similarity, k)\n",
    "        retrieved_db[similarity_name][descriptor_name][\"SPR\"][\"indexes\"] = topk_indices_spr\n",
    "        retrieved_db[similarity_name][descriptor_name][\"SPR\"][\"similarities\"] = topk_similarities_spr\n",
    "\n",
    "\n",
    "        print(f\"Top-{k} for {similarity_name} - {descriptor_name} (SPR):\")\n",
    "        print(f\"Indices: {topk_indices_spr}\\n\\n\")\n",
    "\n",
    "        # BB Top-k retrieval for each partition level\n",
    "        bb_similarity = data_dict[\"BB\"]\n",
    "        for partition_level, distances in bb_similarity.items():\n",
    "            retrieved_db[similarity_name][descriptor_name][\"BB\"][partition_level] = {}\n",
    "            topk_indices_bb, topk_similarities_bb = get_topk_distances(distances, k)\n",
    "            retrieved_db[similarity_name][descriptor_name][\"BB\"][partition_level][\"indexes\"] = topk_indices_bb\n",
    "            retrieved_db[similarity_name][descriptor_name][\"BB\"][partition_level][\"similarities\"] = topk_similarities_bb\n",
    "            print(f\"Top-{k} for {similarity_name} - {descriptor_name} (BB Level {partition_level}):\")\n",
    "            print(f\"Indices: {topk_indices_bb}\\n\\n\")\n"
   ],
   "id": "39241dd02ccc0931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = [MeanAveragePrecisionAtK()]\n",
    "K = [1,5]"
   ],
   "id": "78795597f8071db6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "for i, k in enumerate(K):\n",
    "    for metric in metrics:\n",
    "        for similarity in similarity_classes:\n",
    "            similarity_name = similarity.__class__.__name__\n",
    "            for descriptor in descriptors:\n",
    "                descriptor_name = descriptor.name\n",
    "\n",
    "                # SPR\n",
    "                indexes_retrieved = retrieved_db[similarity_name][descriptor_name][\"SPR\"][\"indexes\"]\n",
    "                map_val = round(metric.compute(GT_QSD1_W1_LIST, indexes_retrieved, k), 2)\n",
    "                results.append({\n",
    "                    \"K\": k,\n",
    "                    \"Metric\": metric.__class__.__name__,\n",
    "                    \"Descriptor\": descriptor_name,\n",
    "                    \"Similarity\": similarity_name,\n",
    "                    \"Method\": f\"SPR\",\n",
    "                    \"Result\": map_val,\n",
    "                    \"Indices\": indexes_retrieved,\n",
    "                })\n",
    "\n",
    "                # BB\n",
    "                for partition_level in partition_levels:\n",
    "                    indexes_retrieved = retrieved_db[similarity_name][descriptor_name][\"BB\"][partition_level][\"indexes\"]\n",
    "                    map_val = round(metric.compute(GT_QSD1_W1_LIST, indexes_retrieved, k), 2)\n",
    "                    results.append({\n",
    "                        \"K\": k,\n",
    "                        \"Metric\": metric.__class__.__name__,\n",
    "                        \"Descriptor\": descriptor_name,\n",
    "                        \"Similarity\": similarity_name,\n",
    "                        \"Method\": f\"BB at level {partition_level}\",\n",
    "                        \"Result\": map_val,\n",
    "                        \"Indices\": indexes_retrieved,\n",
    "                    })\n",
    "\n",
    "\n",
    "# Convert the results into a DataFrame for easy analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Optionally, drop columns you don’t need in the final analysis\n",
    "results_df_cleaned = results_df.drop(columns=[\"Indices\", \"Descriptor_id\", \"Similarity_id\"], errors='ignore')\n",
    "\n",
    "# Output the DataFrame\n",
    "results_df_cleaned"
   ],
   "id": "1675e8ff420f6339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_df_cleaned.to_csv(f\"out_{datetime.datetime.now()}.csv\")",
   "id": "406f2232d27394a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f80dcf0076c9bf72",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
