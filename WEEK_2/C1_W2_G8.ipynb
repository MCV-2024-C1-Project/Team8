{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cc59bf39f6bbae",
   "metadata": {},
   "source": [
    "# C1 W2 Group 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae0579860f63dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "\n",
    "from src.data import GT_QSD1_W2_LIST, GT_QSD2_W2_LIST\n",
    "from src.paths import BBDD_PATH, WEEK_2_RESULTS_PATH, QSD1_W2_PATH, QSD2_W2_PATH, QST2_W2_PATH\n",
    "from src.descriptors import MultiColorSpaceHistogramDescriptor1D, ColorHistogramDescriptor3D, MultiColorSpaceHistogramDescriptor3D\n",
    "from src.similarities import HistogramIntersection, HellingerKernel, Bhattacharyya\n",
    "from src.metrics import MeanAveragePrecisionAtK\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from skimage import filters\n",
    "import cv2\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6eac9dee74801",
   "metadata": {},
   "source": [
    "## Task 1 & 2 - Implement 3D / 2D and block and hierarchical histograms\n",
    "\n",
    "### Data processing : Image loading, partitioning & descriptor computation\n",
    "\n",
    "First, we compute the partitions of the images, and the corresponding descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa4a6530a31f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_image_PIL_list = [Image.open(db_img_path) for db_img_path in sorted(BBDD_PATH.glob(\"*.jpg\"))]  # Load once\n",
    "for idx, db_img in enumerate(database_image_PIL_list):\n",
    "    assert db_img.filename.endswith(f\"{idx}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69993550ce05a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image_PIL_list = [Image.open(query_img_path) for query_img_path in sorted(QSD1_W2_PATH.glob(\"*.jpg\"))]  # Load once\n",
    "for idx, query_img in enumerate(query_image_PIL_list):\n",
    "    assert query_img.filename.endswith(f\"{idx}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e73ae90d5c5b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_image(image: Image.Image, N: int):\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # Compute info about the partition\n",
    "    rows = cols = int(N)  # Assuming N is a perfect square\n",
    "    part_width = img_width // cols\n",
    "    part_height = img_height // rows\n",
    "    \n",
    "    partitions = []\n",
    "    \n",
    "    # Crop each partition\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            left = col * part_width\n",
    "            top = row * part_height\n",
    "            right = left + part_width\n",
    "            bottom = top + part_height\n",
    "            part = image.crop((left, top, right, bottom))\n",
    "            partitions.append(part)\n",
    "    \n",
    "    return partitions\n",
    "\n",
    "def plot_partitions(image, N):\n",
    "    img_width, img_height = image.size\n",
    "    \n",
    "    # Compute info about the partition\n",
    "    rows = cols = int(N)  # Assuming N is a perfect square\n",
    "    part_width = img_width // cols\n",
    "    part_height = img_height // rows\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Vertical partition lines\n",
    "    for i in range(1, cols):\n",
    "        x = i * part_width\n",
    "        draw.line([(x, 0), (x, img_height)], fill=\"white\", width=5)  \n",
    "    \n",
    "    # Horizontal partition lines\n",
    "    for i in range(1, rows):\n",
    "        y = i * part_height\n",
    "        draw.line([(0, y), (img_width, y)], fill=\"white\", width=5)  \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe74ad5e1964da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_images_db = {}\n",
    "partition_levels = [1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b34a0afb5498d10",
   "metadata": {},
   "source": [
    "To make the execution faster we persist the partitions of the images for the next runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7d0439c975970",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition_level in partition_levels:\n",
    "    partition_level_dir = WEEK_2_RESULTS_PATH / f\"partitioned_db_images_level_{partition_level}\"\n",
    "    \n",
    "    # If we have previously executed the code, the partitions are stored in the disk and we directly load them\n",
    "    if partition_level_dir.exists():\n",
    "        partitioned_images_db[partition_level] = []\n",
    "\n",
    "        for img_idx in tqdm(range(len(database_image_PIL_list)), desc=f\"Loading images at level {partition_level}\"):\n",
    "            partitions = []\n",
    "            block_idx = 0\n",
    "            while True:\n",
    "                img_path = partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\"\n",
    "                if not img_path.exists():\n",
    "                    break  \n",
    "                partitions.append(Image.open(img_path))\n",
    "                block_idx += 1\n",
    "\n",
    "            partitioned_images_db[partition_level].append(partitions)\n",
    "\n",
    "        continue\n",
    "    \n",
    "    # Otherwise we compute and persist the partitions for next executions\n",
    "    partition_level_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if partition_level == 1:\n",
    "        print(\"Partitioning at level 1\")\n",
    "        partitioned_images_db[partition_level] = [[img] for img in database_image_PIL_list]\n",
    "    else:\n",
    "        partitioned_images_db[partition_level] = [\n",
    "            partition_image(img, partition_level) \n",
    "            for img in tqdm(database_image_PIL_list, desc=f\"Partitioning at level {partition_level}\")\n",
    "        ]\n",
    "    \n",
    "    for img_idx, partitions in tqdm(enumerate(partitioned_images_db[partition_level]), \n",
    "                                    total=len(partitioned_images_db[partition_level]), \n",
    "                                    desc=f\"Saving images at level {partition_level}\"):\n",
    "        for block_idx, block_img in enumerate(partitions):\n",
    "            block_img.save(partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc930b06647f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = [\n",
    "    MultiColorSpaceHistogramDescriptor1D(['HSV', 'LAB', 'YCbCr'], histogram_type='log-chromatic'),\n",
    "    #ColorHistogramDescriptor3D(\"RGB\", 9, histogram_type='log-chromatic'),\n",
    "    # ColorHistogramDescriptor3D(\"HSV\", 9, histogram_type='log-chromatic'),\n",
    "    # ColorHistogramDescriptor3D(\"YCbCr\", 9, histogram_type='log-chromatic'),\n",
    "    #ColorHistogramDescriptor3D(\"LAB\", 9, histogram_type='log-chromatic'),\n",
    "    MultiColorSpaceHistogramDescriptor3D(['HSV', 'LAB', 'YCbCr'], 6 ,histogram_type='log-chromatic'),\n",
    "]\n",
    "\n",
    "partitioned_histograms_db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cc07b40abb516",
   "metadata": {},
   "outputs": [],
   "source": [
    "for descriptor in descriptors:\n",
    "    print(\"Descriptor: \", descriptor.name)\n",
    "    partitioned_histograms_db[descriptor.name] = {}\n",
    "\n",
    "    for partition_level in partition_levels:\n",
    "        print(\"Partition Level: \", partition_level)\n",
    "        partitioned_histograms_db[descriptor.name][partition_level] = []\n",
    "\n",
    "        for partitions in tqdm(partitioned_images_db[partition_level]):\n",
    "            histograms_img = []\n",
    "            for partition_img in partitions:\n",
    "                histogram_partition = descriptor.compute(partition_img)\n",
    "                histograms_img.append(histogram_partition)\n",
    "\n",
    "\n",
    "            concatenated_histogram = np.concatenate(histograms_img, axis=0)\n",
    "                \n",
    "            partitioned_histograms_db[descriptor.name][partition_level].append(concatenated_histogram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264b832d9b5fc0d",
   "metadata": {},
   "source": [
    "Now we perform a similar process for the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6168c73fbb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_images_query = {}\n",
    "\n",
    "for partition_level in partition_levels:\n",
    "    partition_level_dir = WEEK_2_RESULTS_PATH / f\"partitioned_query_images_level_{partition_level}\"\n",
    "    \n",
    "    # If we have previously executed the code, the partitions are stored in the disk and we directly load them\n",
    "    if partition_level_dir.exists():\n",
    "        partitioned_images_query[partition_level] = []\n",
    "\n",
    "        for img_idx in tqdm(range(len(query_image_PIL_list)), desc=f\"Loading images at level {partition_level}\"):\n",
    "            partitions = []\n",
    "            block_idx = 0\n",
    "            while True:\n",
    "                img_path = partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\"\n",
    "                if not img_path.exists():\n",
    "                    break  \n",
    "                partitions.append(Image.open(img_path))\n",
    "                block_idx += 1\n",
    "\n",
    "            partitioned_images_query[partition_level].append(partitions)\n",
    "\n",
    "        continue\n",
    "    \n",
    "    # Otherwise we compute and persist the partitions for next executions\n",
    "    partition_level_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if partition_level == 1:\n",
    "        print(\"Partitioning at level 1\")\n",
    "        partitioned_images_query[partition_level] = [[img] for img in query_image_PIL_list]\n",
    "    else:\n",
    "        partitioned_images_query[partition_level] = [\n",
    "            partition_image(img, partition_level) \n",
    "            for img in tqdm(query_image_PIL_list, desc=f\"Partitioning at level {partition_level}\")\n",
    "        ]\n",
    "    \n",
    "    for img_idx, partitions in tqdm(enumerate(partitioned_images_query[partition_level]), \n",
    "                                    total=len(partitioned_images_query[partition_level]), \n",
    "                                    desc=f\"Saving images at level {partition_level}\"):\n",
    "        for block_idx, block_img in enumerate(partitions):\n",
    "            block_img.save(partition_level_dir / f\"img_{img_idx}_block_{block_idx}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27fdb432c94a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_histograms_query = {}\n",
    "\n",
    "for descriptor in descriptors:\n",
    "    print(\"Descriptor: \", descriptor.name)\n",
    "    partitioned_histograms_query[descriptor.name] = {}\n",
    "\n",
    "    for partition_level in partition_levels:\n",
    "        print(\"Partition Level: \", partition_level)\n",
    "        partitioned_histograms_query[descriptor.name][partition_level] = []\n",
    "\n",
    "        for partitions in tqdm(partitioned_images_query[partition_level]):\n",
    "            histograms_img = []\n",
    "            for partition_img in partitions:\n",
    "                histogram_partition = descriptor.compute(partition_img)\n",
    "                histograms_img.append(histogram_partition)\n",
    "\n",
    "            concatenated_histogram = np.concatenate(histograms_img, axis=0)\n",
    "            partitioned_histograms_query[descriptor.name][partition_level].append(concatenated_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffa46f6d0e4bed",
   "metadata": {},
   "source": [
    "### Concatentation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc7945f221dde",
   "metadata": {},
   "source": [
    "Now we will concatenate the histograms for both cases, the block-based (BB) and the spatial pyramid representation (SPR). We only do that if the histogram has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76912b791b6fb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms_SPR_DB = {}\n",
    "histograms_SPR_Q = {}\n",
    "histograms_BB_DB = {}\n",
    "histograms_BB_Q = {}\n",
    "\n",
    "for db_data, query_data in zip(partitioned_histograms_db.items(), partitioned_histograms_query.items()):\n",
    "    descriptor_db = db_data[0]\n",
    "    partitions_db = db_data[1]\n",
    "    descriptor_q = query_data[0]\n",
    "    partitions_q = query_data[1]\n",
    "    assert descriptor_db == descriptor_q\n",
    "\n",
    "    histograms_BB_DB[descriptor_db] = partitioned_histograms_db[descriptor_db]\n",
    "    histograms_BB_Q[descriptor_db] = partitioned_histograms_query[descriptor_db]\n",
    "\n",
    "    histograms_SPR_DB[descriptor_db] = []\n",
    "    for img_idx in range(len(database_image_PIL_list)):\n",
    "        image_histograms = []\n",
    "        for partition_level in partition_levels:\n",
    "            image_histograms.append(partitioned_histograms_db[descriptor_db][partition_level][img_idx])\n",
    "\n",
    "        concatenated = np.concatenate(image_histograms, axis=0)\n",
    "        histograms_SPR_DB[descriptor_db].append(concatenated)\n",
    "\n",
    "\n",
    "    histograms_SPR_Q[descriptor_db] = []\n",
    "    for img_idx in range(len(query_image_PIL_list)):\n",
    "        image_histograms = []\n",
    "        for partition_level in partition_levels:\n",
    "            image_histograms.append(partitioned_histograms_query[descriptor_db][partition_level][img_idx])\n",
    "\n",
    "        concatenated = np.concatenate(image_histograms, axis=0)\n",
    "        histograms_SPR_Q[descriptor_db].append(concatenated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8446149c1d823e7",
   "metadata": {},
   "source": [
    "### Similarity computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2774de91152f7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_classes = [\n",
    "    #MSE(),\n",
    "    #L1Distance(),\n",
    "    #ChiSquaredDistance(),\n",
    "    HistogramIntersection(),\n",
    "    HellingerKernel(),\n",
    "    Bhattacharyya()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c2453d4008d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_descriptor_distances_to_db_list = {}\n",
    "\n",
    "for similarity in similarity_classes:\n",
    "    similarity_name = similarity.__class__.__name__\n",
    "    query_descriptor_distances_to_db_list[similarity_name] = {}\n",
    "    for descriptor in descriptors:\n",
    "        descriptor_name = descriptor.name\n",
    "        print(f\"- {similarity_name} & {descriptor_name}\")\n",
    "        query_descriptor_distances_to_db_list[similarity_name][descriptor_name] = {\n",
    "            \"BB\": {}, # We will compute one similarity per partition level\n",
    "            \"SPR\": [] # We will compute one similarity per image\n",
    "        }\n",
    "        query_descriptors = np.array(histograms_SPR_Q[descriptor_name])\n",
    "        database_descriptors = np.array(histograms_SPR_DB[descriptor_name])\n",
    "\n",
    "        # COMPUTE SIMILARITIES\n",
    "        spr_similarity = similarity.compute(query_descriptors, database_descriptors)\n",
    "        query_descriptor_distances_to_db_list[similarity_name][descriptor_name][\"SPR\"] = spr_similarity\n",
    "\n",
    "        for partition_level in partition_levels:\n",
    "            partitioned_database_descriptors = np.array(histograms_BB_DB[descriptor_name][partition_level])\n",
    "            partitioned_query_descriptors = np.array(histograms_BB_Q[descriptor_name][partition_level])\n",
    "            bb_similarity = similarity.compute(partitioned_query_descriptors, partitioned_database_descriptors)\n",
    "            query_descriptor_distances_to_db_list[similarity_name][descriptor_name][\"BB\"][partition_level] = bb_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8654b265adb910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distance_matrix(distance_matrix, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(distance_matrix, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Database Images')\n",
    "    plt.ylabel('Query Images')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af11bc648c60d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for similarity_name, descriptors_dict in query_descriptor_distances_to_db_list.items():\n",
    "    for descriptor_name, data_dict in descriptors_dict.items():\n",
    "        # Plot the SPR distance matrix\n",
    "        spr_similarity = data_dict[\"SPR\"]\n",
    "        plot_distance_matrix(spr_similarity, f\"{similarity_name} - {descriptor_name} - SPR Distance Matrix\")\n",
    "        \n",
    "        # Plot the BB distance matrices for all partition levels\n",
    "        bb_similarity = data_dict[\"BB\"]\n",
    "        for partition_level, distances in bb_similarity.items():\n",
    "            plot_distance_matrix(distances, f\"{similarity_name} - {descriptor_name} - BB Distance Matrix (Partition Level {partition_level})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afaa19673dfe88d",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a4b70f615bf15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_distances(query_distances_to_bbdd: np.array, k: int = 1) -> tuple[list[list], list[list]]:\n",
    "    retrieved_bbdd_indices = np.argsort(query_distances_to_bbdd, axis=1)[:, :k]\n",
    "    \n",
    "    retrieved_bbdd_similarity = np.take_along_axis(query_distances_to_bbdd, retrieved_bbdd_indices, axis=1)\n",
    "    \n",
    "    return retrieved_bbdd_indices.tolist(), retrieved_bbdd_similarity.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39241dd02ccc0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define k (number of top results to retrieve)\n",
    "k = 5\n",
    "\n",
    "retrieved_db = {\n",
    "\n",
    "}\n",
    "\n",
    "for similarity_name, descriptors_dict in query_descriptor_distances_to_db_list.items():\n",
    "    retrieved_db[similarity_name] = {}\n",
    "    for descriptor_name, data_dict in descriptors_dict.items():\n",
    "        print(similarity_name, descriptor_name)\n",
    "        retrieved_db[similarity_name][descriptor_name] = {\n",
    "            \"SPR\": {},\n",
    "            \"BB\": {},\n",
    "        }\n",
    "\n",
    "        # SPR Top-k retrieval\n",
    "        spr_similarity = data_dict[\"SPR\"]\n",
    "        topk_indices_spr, topk_similarities_spr = get_topk_distances(spr_similarity, k)\n",
    "        retrieved_db[similarity_name][descriptor_name][\"SPR\"][\"indexes\"] = topk_indices_spr\n",
    "        retrieved_db[similarity_name][descriptor_name][\"SPR\"][\"similarities\"] = topk_similarities_spr\n",
    "\n",
    "\n",
    "        print(f\"Top-{k} for {similarity_name} - {descriptor_name} (SPR):\")\n",
    "        print(f\"Indices: {topk_indices_spr}\\n\\n\")\n",
    "\n",
    "        # BB Top-k retrieval for each partition level\n",
    "        bb_similarity = data_dict[\"BB\"]\n",
    "        for partition_level, distances in bb_similarity.items():\n",
    "            retrieved_db[similarity_name][descriptor_name][\"BB\"][partition_level] = {}\n",
    "            topk_indices_bb, topk_similarities_bb = get_topk_distances(distances, k)\n",
    "            retrieved_db[similarity_name][descriptor_name][\"BB\"][partition_level][\"indexes\"] = topk_indices_bb\n",
    "            retrieved_db[similarity_name][descriptor_name][\"BB\"][partition_level][\"similarities\"] = topk_similarities_bb\n",
    "            print(f\"Top-{k} for {similarity_name} - {descriptor_name} (BB Level {partition_level}):\")\n",
    "            print(f\"Indices: {topk_indices_bb}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78795597f8071db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [MeanAveragePrecisionAtK()]\n",
    "K = [1,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675e8ff420f6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, k in enumerate(K):\n",
    "    for metric in metrics:\n",
    "        for similarity in similarity_classes:\n",
    "            similarity_name = similarity.__class__.__name__\n",
    "            for descriptor in descriptors:\n",
    "                descriptor_name = descriptor.name\n",
    "\n",
    "                # SPR\n",
    "                indexes_retrieved = retrieved_db[similarity_name][descriptor_name][\"SPR\"][\"indexes\"]\n",
    "                map_val = round(metric.compute(GT_QSD1_W2_LIST, indexes_retrieved, k), 2)\n",
    "                results.append({\n",
    "                    \"K\": k,\n",
    "                    \"Metric\": metric.__class__.__name__,\n",
    "                    \"Descriptor\": descriptor_name,\n",
    "                    \"Similarity\": similarity_name,\n",
    "                    \"Method\": f\"SPR\",\n",
    "                    \"Result\": map_val,\n",
    "                    \"Indices\": indexes_retrieved,\n",
    "                })\n",
    "\n",
    "                # BB\n",
    "                for partition_level in partition_levels:\n",
    "                    indexes_retrieved = retrieved_db[similarity_name][descriptor_name][\"BB\"][partition_level][\"indexes\"]\n",
    "                    map_val = round(metric.compute(GT_QSD1_W2_LIST, indexes_retrieved, k), 2)\n",
    "                    results.append({\n",
    "                        \"K\": k,\n",
    "                        \"Metric\": metric.__class__.__name__,\n",
    "                        \"Descriptor\": descriptor_name,\n",
    "                        \"Similarity\": similarity_name,\n",
    "                        \"Method\": f\"BB at level {partition_level}\",\n",
    "                        \"Result\": map_val,\n",
    "                        \"Indices\": indexes_retrieved,\n",
    "                    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df_cleaned = results_df.drop(columns=[\"Indices\", \"Descriptor_id\", \"Similarity_id\"], errors='ignore')\n",
    "\n",
    "results_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406f2232d27394a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_cleaned.to_csv(f\"out_{datetime.datetime.now()}.csv\".replace(':','-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2af128",
   "metadata": {},
   "source": [
    "## Task 3 - Remove background using the background color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ae14bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2_image_PIL_list = [Image.open(db_img_path) for db_img_path in sorted(QSD2_W2_PATH.glob(\"*.jpg\"))]  # Load once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbfb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_rgb(query_image_PIL, pctg=0.5):  \n",
    "    width, height = query_image_PIL.size\n",
    "    \n",
    "    crop_width = int(pctg * width)\n",
    "    crop_height = int(pctg * height)\n",
    "    \n",
    "    left = (width - crop_width) // 2\n",
    "    top = (height - crop_height) // 2\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    crop_box = (left, top, right, bottom)\n",
    "    \n",
    "    central_region = query_image_PIL.crop(crop_box)\n",
    "    \n",
    "    central_region_rgb = np.array(central_region)\n",
    "    \n",
    "    central_region_gray = np.mean(central_region_rgb, axis=2)  # Average the RGB channels\n",
    "    \n",
    "    # Otsu's threshold\n",
    "    otsu_threshold = filters.threshold_otsu(central_region_gray)\n",
    "    \n",
    "    # Count dark, neutral, and bright pixels based on RGB values\n",
    "    dark_threshold = otsu_threshold  \n",
    "    bright_threshold = otsu_threshold  \n",
    "    \n",
    "    dark_pixels = np.all(central_region_rgb <= dark_threshold, axis=-1) \n",
    "    bright_pixels = np.all(central_region_rgb >= bright_threshold, axis=-1) \n",
    "    \n",
    "    num_dark_pixels = np.sum(dark_pixels)\n",
    "    num_bright_pixels = np.sum(bright_pixels)\n",
    "    num_total_pixels = central_region_rgb.shape[0] * central_region_rgb.shape[1]\n",
    "    \n",
    "    dark_percentage = num_dark_pixels / num_total_pixels\n",
    "    bright_percentage = num_bright_pixels / num_total_pixels\n",
    "    \n",
    "    # Classify based on the percentages\n",
    "    if dark_percentage > 0.60 > bright_percentage:\n",
    "        return \"dark\"\n",
    "    elif bright_percentage > 0.50 > dark_percentage:\n",
    "        return \"bright\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "    \n",
    "for query_image_path, query_image_PIL in zip(sorted(QSD2_W2_PATH.glob(\"*.jpg\")), query2_image_PIL_list):\n",
    "    print(f\"{query_image_path.stem}: {classify_image_rgb(query_image_PIL)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad7fffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_otsu_threshold(channel):\n",
    "    channel = np.array(channel)\n",
    "    blurred_channel = cv2.GaussianBlur(channel, (11, 11), 0) \n",
    "    threshold_value, _ = cv2.threshold(blurred_channel, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return (channel > threshold_value).astype(np.uint8) * 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(query2_image_PIL_list)\n",
    "fig, axes = plt.subplots(num_images, 2, figsize=(5, 2.5 * num_images))  # Adjust height based on number of images\n",
    "\n",
    "for i, (query_image_path, query_image_PIL) in enumerate(zip(sorted(QSD2_W2_PATH.glob(\"*.jpg\")), query2_image_PIL_list)):\n",
    "    image_classification = classify_image_rgb(query_image_PIL) \n",
    "\n",
    "    if image_classification == \"dark\":\n",
    "        best_image_channel_mask = apply_otsu_threshold(np.array(query_image_PIL.convert('LAB').split()[0]))  \n",
    "    elif image_classification == \"bright\":\n",
    "        best_image_channel_mask = apply_otsu_threshold(np.array(query_image_PIL.convert('LAB').split()[2]))  \n",
    "    else:\n",
    "        best_image_channel_mask = apply_otsu_threshold(np.array(query_image_PIL.convert('HSV').split()[1]))  \n",
    "    \n",
    "    # Display original image and mask in the same figure\n",
    "    axes[i, 0].imshow(np.array(query_image_PIL))  \n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(best_image_channel_mask, cmap='gray')  \n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 0].set_title(f\"{query_image_path.stem} - {image_classification}\", fontsize=10)\n",
    "\n",
    "plt.subplots_adjust(hspace=.1,wspace=.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64453fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(query_image_PIL, image_classification):\n",
    "    if image_classification == \"neutral\":\n",
    "        _, s, _ = query_image_PIL.convert('HSV').split()  \n",
    "        th = apply_otsu_threshold(s)  \n",
    "        w = th.shape[1]  \n",
    "        \n",
    "        _, _, stats, _ = cv2.connectedComponentsWithStats(th, connectivity=8)\n",
    "        stats = stats[1:]  \n",
    "        largest_label = np.argmax(stats[:, cv2.CC_STAT_AREA])  \n",
    "        wh = stats[largest_label] \n",
    "        width = wh[cv2.CC_STAT_WIDTH]\n",
    "        pp = width / w  \n",
    "\n",
    "        if pp < 0.55: # apply dilatation !\n",
    "            kernel_size = 5\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size)) # Square with dimensions kernel_size x kernel_size\n",
    "            th = cv2.dilate(th, kernel, iterations=3)\n",
    "            _, _, stats, _ = cv2.connectedComponentsWithStats(th, connectivity=8)\n",
    "            stats = stats[1:]\n",
    "            largest_label = np.argmax(stats[:, cv2.CC_STAT_AREA])\n",
    "            wh = stats[largest_label]\n",
    "    \n",
    "    else: # LAB\n",
    "        l, _, b = query_image_PIL.convert('LAB').split() \n",
    "        if image_classification == \"dark\": # L\n",
    "            th = apply_otsu_threshold(l) \n",
    "        else: # light, B\n",
    "            th = apply_otsu_threshold(b) \n",
    "        inverted_mask = cv2.bitwise_not(th) \n",
    "\n",
    "        _, _, stats, _ = cv2.connectedComponentsWithStats(inverted_mask, connectivity=8)\n",
    "        stats = stats[1:]  \n",
    "        largest_label = np.argmax(stats[:, cv2.CC_STAT_AREA])\n",
    "        wh = stats[largest_label]\n",
    "\n",
    "    x = wh[cv2.CC_STAT_LEFT]\n",
    "    y = wh[cv2.CC_STAT_TOP]\n",
    "    width = wh[cv2.CC_STAT_WIDTH]\n",
    "    height = wh[cv2.CC_STAT_HEIGHT]\n",
    "    cleaned_mask = np.zeros_like(th) \n",
    "    cleaned_mask[y:y + height, x:x + width] = 255 \n",
    "\n",
    "    return cleaned_mask  \n",
    "\n",
    "masks = []\n",
    "for query_image_path, query_image_PIL in zip(sorted(BBDD_PATH.glob(\"*.jpg\")), query2_image_PIL_list):\n",
    "    image_classification = classify_image_rgb(query_image_PIL)  \n",
    "    mask = get_mask(query_image_PIL, image_classification)  \n",
    "    masks.append(mask)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b74ba2",
   "metadata": {},
   "source": [
    "## Task 4 & 5 - Background removal evaluation for QSD2-W2\n",
    "- Precision, recall, F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mask_PIL_list = [Image.open(query_img_path) for query_img_path in sorted(QSD2_W2_PATH.glob(\"*.png\"))]\n",
    "mask_gt = [np.array(msk) for msk in query_mask_PIL_list] \n",
    "\n",
    "def calculate_metrics(pred_mask, true_mask):\n",
    "    pred_mask_flat = pred_mask.flatten()\n",
    "    true_mask_flat = true_mask.flatten()\n",
    "\n",
    "    pred_mask_flat = (pred_mask_flat > 127).astype(int)\n",
    "    true_mask_flat = (true_mask_flat > 127).astype(int)\n",
    "\n",
    "    precision = precision_score(true_mask_flat, pred_mask_flat)\n",
    "    recall = recall_score(true_mask_flat, pred_mask_flat)\n",
    "    f1 = f1_score(true_mask_flat, pred_mask_flat)\n",
    "\n",
    "    return precision, recall, f1  \n",
    "\n",
    "total_precision, total_recall, total_f1 = 0, 0, 0\n",
    "\n",
    "num_masks = len(masks)  \n",
    "\n",
    "for pred_mask, true_mask in zip(masks, mask_gt):\n",
    "    precision, recall, f1 = calculate_metrics(pred_mask, true_mask)  \n",
    "    total_precision += precision  \n",
    "    total_recall += recall \n",
    "    total_f1 += f1 \n",
    "\n",
    "avg_precision = total_precision / num_masks\n",
    "avg_recall = total_recall / num_masks\n",
    "avg_f1 = total_f1 / num_masks\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72004264",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_PIL_list = [Image.open(query_img_path) for query_img_path in sorted(QST2_W2_PATH.glob(\"*.jpg\"))]\n",
    "test_masks = []  \n",
    "\n",
    "for test_image_path, test_image_PIL in zip(sorted(QST2_W2_PATH.glob(\"*.jpg\")), test_image_PIL_list):\n",
    "    image_classification = classify_image_rgb(test_image_PIL) \n",
    "    mask = get_mask(test_image_PIL, image_classification) \n",
    "    test_masks.append(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602d876",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
